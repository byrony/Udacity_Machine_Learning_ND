{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "#from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.cross_validation import KFold\n",
    "from scipy.stats import skew, boxcox\n",
    "from sklearn import preprocessing\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we create the new features before, so read the transformed data\n",
    "# the transformed data is too large, so didn't upload them to github\n",
    "train = pd.read_csv('train_clear.csv')\n",
    "test = pd.read_csv('test_clear.csv')\n",
    "\n",
    "train_test = pd.concat((train, test)).reset_index(drop=True)\n",
    "\n",
    "#process additional features according to Ali.\n",
    "# https://www.kaggle.com/aliajouz/allstate-claims-severity/singel-model-lb-1117/run/413221/code\n",
    "train_test[\"cont1\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont1\"]))\n",
    "train_test[\"cont4\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont4\"]))\n",
    "train_test[\"cont5\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont5\"]))\n",
    "train_test[\"cont8\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont8\"]))\n",
    "train_test[\"cont10\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont10\"]))\n",
    "train_test[\"cont11\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont11\"]))\n",
    "train_test[\"cont12\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont12\"]))\n",
    "\n",
    "train_test[\"cont6\"] = np.log(preprocessing.minmax_scale(train_test[\"cont6\"]) + 0000.1)\n",
    "train_test[\"cont7\"] = np.log(preprocessing.minmax_scale(train_test[\"cont7\"]) + 0000.1)\n",
    "train_test[\"cont9\"] = np.log(preprocessing.minmax_scale(train_test[\"cont9\"]) + 0000.1)\n",
    "train_test[\"cont13\"] = np.log(preprocessing.minmax_scale(train_test[\"cont13\"]) + 0000.1)\n",
    "train_test[\"cont14\"] = (np.maximum(train_test[\"cont14\"] - 0.179722, 0) / 0.665122) ** 0.25\n",
    "\n",
    "train = train_test.iloc[:train.shape[0], :]\n",
    "test = train_test.iloc[train.shape[0]:, :]\n",
    "X_train = train.drop(['id', 'loss'], axis=1)\n",
    "y_train = train['loss']\n",
    "ids = test['id'].values\n",
    "X_test = test.drop(['id', 'loss'], axis=1)\n",
    "\n",
    "# del train\n",
    "# del test\n",
    "# del train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shift = 200\n",
    "y_train = np.log(y_train + shift)\n",
    "\n",
    "# define object function and evaluation metric\n",
    "\n",
    "# fair_constant = 0.7\n",
    "# def fair_obj(preds, dtrain):\n",
    "#     labels = dtrain.get_label()\n",
    "#     x = (preds - labels)\n",
    "#     den = abs(x) + fair_constant\n",
    "#     grad = fair_constant * x / (den)\n",
    "#     hess = fair_constant * fair_constant / (den * den)\n",
    "#     return grad, hess\n",
    "\n",
    "def xg_eval_mae(yhat, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(y)-shift,\n",
    "                                      np.exp(yhat)-shift)\n",
    "\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        tmin, tsec = divmod((datetime.now() - start_time).total_seconds(), 60)\n",
    "        print(' Time taken: %i minutes and %s seconds.' %\n",
    "              (tmin, round(tsec, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit the model\n",
    "n_folds = 5\n",
    "cv_sum = 0\n",
    "early_stopping = 100\n",
    "fred = []\n",
    "xgb_rounds = []\n",
    "\n",
    "#d_train_full = xgb.DMatrix(X_train, label=y_train)\n",
    "#d_train_full = xgb.DMatrix(X_train.iloc[0:5000], label=y_train.iloc[0:5000])\n",
    "d_test = xgb.DMatrix(X_test)\n",
    "\n",
    "# define the parameters for xgboost\n",
    "params = {}\n",
    "params['booster'] = 'gbtree'\n",
    "params['objective'] = \"reg:linear\"\n",
    "params['eta'] = 0.03\n",
    "params['gamma'] = 2\n",
    "params['min_child_weight'] = 10\n",
    "params['colsample_bytree'] = 0.8\n",
    "params['subsample'] = 0.8\n",
    "params['max_depth'] = 13\n",
    "params['silent'] = 1\n",
    "\n",
    "params['eval_metric'] = 'mae'\n",
    "\n",
    "#params['random_state'] = 1989 # remove the random state to make each fold more different\n",
    "#params['base_score'] = 2\n",
    "\n",
    "# define a function for the pursose of change training set for trial.\n",
    "\n",
    "\n",
    "# In the training, implement k-folds cross validation.\n",
    "def xgb_model(X_train, y_train=y_train, ids=ids, n_folds=n_folds, cv_sum=cv_sum, early_stopping=early_stopping,\n",
    "              fred=fred, xgb_rounds=xgb_rounds, params=params):\n",
    "    \n",
    "    start_time = timer(None)\n",
    "    \n",
    "    # when input folds is greater than 1, implement k-folds cross validation\n",
    "    # each time use the k-1 folds data to train model, and the rest 1 fold to validation\n",
    "    # repeat the process for each fold.\n",
    "    \n",
    "    if n_folds > 1:\n",
    "        kf = KFold(X_train.shape[0], n_folds = n_folds, random_state=1989)\n",
    "        for i, (train_index, test_index) in enumerate(kf):\n",
    "            print('\\n Fold %d\\n' % (i+1))\n",
    "            X_tra, X_val = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "            y_tra, y_val = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "            d_train = xgb.DMatrix(X_tra, label=y_tra)\n",
    "            d_valid = xgb.DMatrix(X_val, label=y_val)\n",
    "            watchlist = [(d_train, 'train'), (d_valid, 'eval')]\n",
    "\n",
    "            clf = xgb.train(params,\n",
    "                           d_train,\n",
    "                           3000,\n",
    "                           watchlist,\n",
    "                           verbose_eval = 50,\n",
    "                           obj = fair_obj,\n",
    "                           feval = xg_eval_mae,\n",
    "                           early_stopping_rounds = early_stopping)\n",
    "\n",
    "            xgb_rounds.append(clf.best_iteration)\n",
    "            score_val = clf.predict(d_valid, ntree_limit=clf.best_ntree_limit)\n",
    "            cv_score = mean_absolute_error(np.exp(y_val), np.exp(score_val))\n",
    "            print('eval-MAE: %.6f' % cv_score)\n",
    "\n",
    "            # use the model of each fold to predict the test set.\n",
    "            y_pred = np.exp(clf.predict(d_test, ntree_limit=clf.best_ntree_limit)) - shift\n",
    "\n",
    "            if i > 0:\n",
    "                fpred = pred + y_pred\n",
    "            else:\n",
    "                fpred = y_pred\n",
    "            pred = fpred\n",
    "            cv_sum = cv_sum + cv_score\n",
    "\n",
    "        # compute the average prediction of k folds models.\n",
    "        mpred = pred / n_folds # mpred is the k-folds average prediction of test data\n",
    "        score = cv_sum / n_folds\n",
    "        print('\\n Average eval-MAE: %.6f' % score)\n",
    "        n_rounds = int(np.mean(xgb_rounds))\n",
    "\n",
    "        timer(start_time)\n",
    "\n",
    "        ## write results to file\n",
    "        print(\"#\\n Writing results\")\n",
    "        result = pd.DataFrame(mpred, columns=['loss'])\n",
    "        result[\"id\"] = ids\n",
    "        result = result.set_index(\"id\")\n",
    "        print(\"\\n %d-fold average prediction:\\n\" % n_folds)\n",
    "        print(result.head())\n",
    "\n",
    "        now = datetime.now()\n",
    "        score = str(round((cv_sum / n_folds), 6))\n",
    "        sub_file = 'submission_' + str(n_folds) +'fold-average-xgb_' + str(score) + '_' + str(\n",
    "            now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "        print(\"\\n Writing submission: %s\" % sub_file)\n",
    "        result.to_csv(sub_file, index=True, index_label='id')\n",
    "\n",
    "    # if there is no k-folds, training on full data set directly. \n",
    "    elif n_folds == 1:\n",
    "        watchlist = [(d_train_full, 'train_full')]\n",
    "        clf = xgb.train(params,\n",
    "                       d_train_full,\n",
    "                       1010,\n",
    "                       watchlist,\n",
    "                       verbose_eval = 50,\n",
    "                       obj = fair_obj,\n",
    "                       feval = xg_eval_mae)\n",
    "                       #early_stopping_rounds = early_stopping)\n",
    "        y_pred = np.exp(clf.predict(d_test, ntree_limit=clf.best_ntree_limit)) - shift\n",
    "\n",
    "        timer(start_time)\n",
    "        now = datetime.now()\n",
    "        result = pd.DataFrame(y_pred, columns=['loss'])\n",
    "        result[\"id\"] = ids\n",
    "        result = result.set_index(\"id\")\n",
    "        \n",
    "        sub_file = 'submission_' + str(n_folds) +'fold-average-xgb_' + '_' + str(\n",
    "            now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "        print(\"\\n Writing submission: %s\" % sub_file)\n",
    "        result.to_csv(sub_file, index=True, index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Fold 1\n",
      "\n",
      "[0]\ttrain-rmse:4.88939\teval-rmse:4.88568\ttrain-mae:3218.75\teval-mae:3212.62\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.550661\teval-rmse:0.553594\ttrain-mae:1316.01\teval-mae:1319.5\n",
      "[100]\ttrain-rmse:0.479979\teval-rmse:0.489293\ttrain-mae:1148.86\teval-mae:1171.56\n",
      "[150]\ttrain-rmse:0.471719\teval-rmse:0.483724\ttrain-mae:1123.73\teval-mae:1155.52\n",
      "[200]\ttrain-rmse:0.46798\teval-rmse:0.481857\ttrain-mae:1111.77\teval-mae:1149.8\n",
      "[250]\ttrain-rmse:0.465553\teval-rmse:0.480749\ttrain-mae:1103.92\teval-mae:1146.3\n",
      "[300]\ttrain-rmse:0.463417\teval-rmse:0.479888\ttrain-mae:1097.29\teval-mae:1143.76\n",
      "[350]\ttrain-rmse:0.461777\teval-rmse:0.479287\ttrain-mae:1092.11\teval-mae:1141.85\n",
      "[400]\ttrain-rmse:0.460228\teval-rmse:0.478797\ttrain-mae:1087.37\teval-mae:1140.2\n",
      "[450]\ttrain-rmse:0.459082\teval-rmse:0.478459\ttrain-mae:1083.79\teval-mae:1139.29\n",
      "[500]\ttrain-rmse:0.458068\teval-rmse:0.47819\ttrain-mae:1080.54\teval-mae:1138.37\n",
      "[550]\ttrain-rmse:0.457103\teval-rmse:0.478066\ttrain-mae:1077.45\teval-mae:1138.04\n",
      "[600]\ttrain-rmse:0.456301\teval-rmse:0.477886\ttrain-mae:1074.92\teval-mae:1137.54\n",
      "[650]\ttrain-rmse:0.455447\teval-rmse:0.477716\ttrain-mae:1072.15\teval-mae:1137.07\n",
      "[700]\ttrain-rmse:0.454641\teval-rmse:0.477575\ttrain-mae:1069.62\teval-mae:1136.63\n",
      "[750]\ttrain-rmse:0.453839\teval-rmse:0.477494\ttrain-mae:1067.23\teval-mae:1136.43\n",
      "[800]\ttrain-rmse:0.45309\teval-rmse:0.477334\ttrain-mae:1064.92\teval-mae:1136\n",
      "[850]\ttrain-rmse:0.452483\teval-rmse:0.477227\ttrain-mae:1062.9\teval-mae:1135.68\n",
      "[900]\ttrain-rmse:0.451846\teval-rmse:0.477182\ttrain-mae:1060.92\teval-mae:1135.51\n",
      "[950]\ttrain-rmse:0.451183\teval-rmse:0.477083\ttrain-mae:1058.93\teval-mae:1135.3\n",
      "[1000]\ttrain-rmse:0.450459\teval-rmse:0.477\ttrain-mae:1056.72\teval-mae:1135.06\n",
      "[1050]\ttrain-rmse:0.449902\teval-rmse:0.476935\ttrain-mae:1054.93\teval-mae:1134.81\n",
      "[1100]\ttrain-rmse:0.449371\teval-rmse:0.476919\ttrain-mae:1053.25\teval-mae:1134.64\n",
      "[1150]\ttrain-rmse:0.448851\teval-rmse:0.476864\ttrain-mae:1051.64\teval-mae:1134.43\n",
      "[1200]\ttrain-rmse:0.448409\teval-rmse:0.476845\ttrain-mae:1050.23\teval-mae:1134.37\n",
      "[1250]\ttrain-rmse:0.447928\teval-rmse:0.476813\ttrain-mae:1048.75\teval-mae:1134.3\n",
      "[1300]\ttrain-rmse:0.447494\teval-rmse:0.47679\ttrain-mae:1047.43\teval-mae:1134.24\n",
      "[1350]\ttrain-rmse:0.447065\teval-rmse:0.476784\ttrain-mae:1046.12\teval-mae:1134.27\n",
      "[1400]\ttrain-rmse:0.44653\teval-rmse:0.476766\ttrain-mae:1044.47\teval-mae:1134.26\n",
      "Stopping. Best iteration:\n",
      "[1311]\ttrain-rmse:0.447384\teval-rmse:0.47678\ttrain-mae:1047.08\teval-mae:1134.18\n",
      "\n",
      "eval-MAE: 1134.174900\n",
      "\n",
      " Fold 2\n",
      "\n",
      "[0]\ttrain-rmse:4.8881\teval-rmse:4.88885\ttrain-mae:3217.3\teval-mae:3218.31\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.550963\teval-rmse:0.556442\ttrain-mae:1315.65\teval-mae:1328.99\n",
      "[100]\ttrain-rmse:0.480313\teval-rmse:0.488729\ttrain-mae:1148.33\teval-mae:1172.25\n",
      "[150]\ttrain-rmse:0.47169\teval-rmse:0.482179\ttrain-mae:1122.48\teval-mae:1153.26\n",
      "[200]\ttrain-rmse:0.468046\teval-rmse:0.480182\ttrain-mae:1110.9\teval-mae:1146.85\n",
      "[250]\ttrain-rmse:0.465451\teval-rmse:0.47902\ttrain-mae:1102.69\teval-mae:1143.29\n",
      "[300]\ttrain-rmse:0.463297\teval-rmse:0.478191\ttrain-mae:1095.86\teval-mae:1140.66\n",
      "[350]\ttrain-rmse:0.461576\teval-rmse:0.477719\ttrain-mae:1090.48\teval-mae:1139.06\n",
      "[400]\ttrain-rmse:0.46028\teval-rmse:0.477343\ttrain-mae:1086.41\teval-mae:1137.91\n",
      "[450]\ttrain-rmse:0.458993\teval-rmse:0.4771\ttrain-mae:1082.48\teval-mae:1137.09\n",
      "[500]\ttrain-rmse:0.457924\teval-rmse:0.476797\ttrain-mae:1079.12\teval-mae:1136.16\n",
      "[550]\ttrain-rmse:0.456922\teval-rmse:0.476577\ttrain-mae:1076\teval-mae:1135.49\n",
      "[600]\ttrain-rmse:0.455971\teval-rmse:0.476435\ttrain-mae:1073\teval-mae:1134.9\n",
      "[650]\ttrain-rmse:0.455085\teval-rmse:0.476287\ttrain-mae:1070.27\teval-mae:1134.47\n",
      "[700]\ttrain-rmse:0.454291\teval-rmse:0.476178\ttrain-mae:1067.79\teval-mae:1134.06\n",
      "[750]\ttrain-rmse:0.45345\teval-rmse:0.476021\ttrain-mae:1065.26\teval-mae:1133.73\n",
      "[800]\ttrain-rmse:0.452754\teval-rmse:0.475922\ttrain-mae:1063.07\teval-mae:1133.45\n",
      "[850]\ttrain-rmse:0.452191\teval-rmse:0.475862\ttrain-mae:1061.32\teval-mae:1133.41\n",
      "[900]\ttrain-rmse:0.451553\teval-rmse:0.475785\ttrain-mae:1059.44\teval-mae:1133.17\n",
      "[950]\ttrain-rmse:0.450974\teval-rmse:0.475726\ttrain-mae:1057.59\teval-mae:1132.98\n",
      "[1000]\ttrain-rmse:0.450348\teval-rmse:0.475675\ttrain-mae:1055.64\teval-mae:1132.74\n",
      "[1050]\ttrain-rmse:0.449814\teval-rmse:0.475617\ttrain-mae:1054.09\teval-mae:1132.56\n",
      "[1100]\ttrain-rmse:0.449242\teval-rmse:0.475538\ttrain-mae:1052.44\teval-mae:1132.33\n",
      "[1150]\ttrain-rmse:0.448779\teval-rmse:0.47546\ttrain-mae:1050.95\teval-mae:1132.1\n",
      "[1200]\ttrain-rmse:0.448301\teval-rmse:0.475437\ttrain-mae:1049.42\teval-mae:1131.95\n",
      "[1250]\ttrain-rmse:0.44784\teval-rmse:0.475383\ttrain-mae:1047.93\teval-mae:1131.89\n",
      "[1300]\ttrain-rmse:0.447364\teval-rmse:0.475365\ttrain-mae:1046.43\teval-mae:1131.85\n",
      "[1350]\ttrain-rmse:0.446891\teval-rmse:0.475329\ttrain-mae:1044.99\teval-mae:1131.72\n",
      "[1400]\ttrain-rmse:0.446473\teval-rmse:0.475288\ttrain-mae:1043.71\teval-mae:1131.64\n",
      "[1450]\ttrain-rmse:0.446036\teval-rmse:0.475261\ttrain-mae:1042.33\teval-mae:1131.48\n",
      "[1500]\ttrain-rmse:0.445569\teval-rmse:0.475256\ttrain-mae:1040.94\teval-mae:1131.48\n",
      "[1550]\ttrain-rmse:0.445102\teval-rmse:0.475266\ttrain-mae:1039.47\teval-mae:1131.46\n",
      "[1600]\ttrain-rmse:0.444708\teval-rmse:0.475237\ttrain-mae:1038.22\teval-mae:1131.31\n",
      "[1650]\ttrain-rmse:0.444269\teval-rmse:0.475246\ttrain-mae:1036.9\teval-mae:1131.29\n",
      "[1700]\ttrain-rmse:0.443873\teval-rmse:0.475237\ttrain-mae:1035.69\teval-mae:1131.34\n",
      "Stopping. Best iteration:\n",
      "[1626]\ttrain-rmse:0.444467\teval-rmse:0.475227\ttrain-mae:1037.55\teval-mae:1131.25\n",
      "\n",
      "eval-MAE: 1131.244807\n",
      "\n",
      " Fold 3\n",
      "\n",
      "[0]\ttrain-rmse:4.88826\teval-rmse:4.89188\ttrain-mae:3214.95\teval-mae:3227.83\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.54968\teval-rmse:0.562233\ttrain-mae:1311.79\teval-mae:1342.52\n",
      "[100]\ttrain-rmse:0.479016\teval-rmse:0.494266\ttrain-mae:1145.18\teval-mae:1186.76\n",
      "[150]\ttrain-rmse:0.470486\teval-rmse:0.487587\ttrain-mae:1119.56\teval-mae:1168.1\n",
      "[200]\ttrain-rmse:0.466799\teval-rmse:0.485526\ttrain-mae:1107.87\teval-mae:1161.75\n",
      "[250]\ttrain-rmse:0.464076\teval-rmse:0.484118\ttrain-mae:1099.3\teval-mae:1157.59\n",
      "[300]\ttrain-rmse:0.462346\teval-rmse:0.483348\ttrain-mae:1093.85\teval-mae:1155.14\n",
      "[350]\ttrain-rmse:0.460801\teval-rmse:0.482799\ttrain-mae:1089.04\teval-mae:1153.56\n",
      "[400]\ttrain-rmse:0.459493\teval-rmse:0.482333\ttrain-mae:1084.97\teval-mae:1152.14\n",
      "[450]\ttrain-rmse:0.458189\teval-rmse:0.481975\ttrain-mae:1081.06\teval-mae:1151.08\n",
      "[500]\ttrain-rmse:0.457131\teval-rmse:0.48175\ttrain-mae:1077.69\teval-mae:1150.21\n",
      "[550]\ttrain-rmse:0.456047\teval-rmse:0.481518\ttrain-mae:1074.28\teval-mae:1149.34\n",
      "[600]\ttrain-rmse:0.455116\teval-rmse:0.481309\ttrain-mae:1071.44\teval-mae:1148.7\n",
      "[650]\ttrain-rmse:0.454235\teval-rmse:0.481146\ttrain-mae:1068.71\teval-mae:1148.14\n",
      "[700]\ttrain-rmse:0.453499\teval-rmse:0.480996\ttrain-mae:1066.4\teval-mae:1147.56\n",
      "[750]\ttrain-rmse:0.452736\teval-rmse:0.480912\ttrain-mae:1064.05\teval-mae:1147.26\n",
      "[800]\ttrain-rmse:0.452054\teval-rmse:0.480843\ttrain-mae:1061.84\teval-mae:1147.07\n",
      "[850]\ttrain-rmse:0.451367\teval-rmse:0.480736\ttrain-mae:1059.82\teval-mae:1146.73\n",
      "[900]\ttrain-rmse:0.450685\teval-rmse:0.480619\ttrain-mae:1057.68\teval-mae:1146.25\n",
      "[950]\ttrain-rmse:0.450087\teval-rmse:0.48055\ttrain-mae:1055.87\teval-mae:1145.97\n",
      "[1000]\ttrain-rmse:0.449492\teval-rmse:0.480455\ttrain-mae:1054.01\teval-mae:1145.62\n",
      "[1050]\ttrain-rmse:0.448972\teval-rmse:0.480349\ttrain-mae:1052.48\teval-mae:1145.4\n",
      "[1100]\ttrain-rmse:0.448435\teval-rmse:0.480285\ttrain-mae:1050.87\teval-mae:1145.3\n",
      "[1150]\ttrain-rmse:0.44789\teval-rmse:0.480272\ttrain-mae:1049.17\teval-mae:1145.19\n",
      "[1200]\ttrain-rmse:0.447455\teval-rmse:0.480232\ttrain-mae:1047.78\teval-mae:1145\n",
      "[1250]\ttrain-rmse:0.446953\teval-rmse:0.480228\ttrain-mae:1046.33\teval-mae:1144.94\n",
      "[1300]\ttrain-rmse:0.446457\teval-rmse:0.480191\ttrain-mae:1044.78\teval-mae:1144.78\n",
      "[1350]\ttrain-rmse:0.446009\teval-rmse:0.480166\ttrain-mae:1043.35\teval-mae:1144.7\n",
      "[1400]\ttrain-rmse:0.445617\teval-rmse:0.48012\ttrain-mae:1042.15\teval-mae:1144.54\n",
      "[1450]\ttrain-rmse:0.445162\teval-rmse:0.480091\ttrain-mae:1040.75\teval-mae:1144.37\n",
      "[1500]\ttrain-rmse:0.444694\teval-rmse:0.480048\ttrain-mae:1039.44\teval-mae:1144.31\n",
      "[1550]\ttrain-rmse:0.444308\teval-rmse:0.480013\ttrain-mae:1038.33\teval-mae:1144.21\n",
      "[1600]\ttrain-rmse:0.443975\teval-rmse:0.479958\ttrain-mae:1037.34\teval-mae:1144.04\n",
      "[1650]\ttrain-rmse:0.443625\teval-rmse:0.479916\ttrain-mae:1036.32\teval-mae:1143.95\n",
      "[1700]\ttrain-rmse:0.443264\teval-rmse:0.479879\ttrain-mae:1035.22\teval-mae:1143.8\n",
      "[1750]\ttrain-rmse:0.442858\teval-rmse:0.479862\ttrain-mae:1034.03\teval-mae:1143.72\n",
      "[1800]\ttrain-rmse:0.442434\teval-rmse:0.479852\ttrain-mae:1032.72\teval-mae:1143.52\n",
      "[1850]\ttrain-rmse:0.44208\teval-rmse:0.479837\ttrain-mae:1031.71\teval-mae:1143.46\n",
      "[1900]\ttrain-rmse:0.441763\teval-rmse:0.479806\ttrain-mae:1030.81\teval-mae:1143.39\n",
      "[1950]\ttrain-rmse:0.441427\teval-rmse:0.479799\ttrain-mae:1029.8\teval-mae:1143.32\n",
      "[2000]\ttrain-rmse:0.441091\teval-rmse:0.479786\ttrain-mae:1028.77\teval-mae:1143.29\n",
      "[2050]\ttrain-rmse:0.440765\teval-rmse:0.479771\ttrain-mae:1027.77\teval-mae:1143.21\n",
      "[2100]\ttrain-rmse:0.440459\teval-rmse:0.479766\ttrain-mae:1026.87\teval-mae:1143.17\n",
      "[2150]\ttrain-rmse:0.440109\teval-rmse:0.47975\ttrain-mae:1025.76\teval-mae:1143.12\n",
      "[2200]\ttrain-rmse:0.439804\teval-rmse:0.479756\ttrain-mae:1024.89\teval-mae:1143.19\n",
      "Stopping. Best iteration:\n",
      "[2132]\ttrain-rmse:0.440248\teval-rmse:0.47975\ttrain-mae:1026.2\teval-mae:1143.09\n",
      "\n",
      "eval-MAE: 1143.086620\n",
      "\n",
      " Fold 4\n",
      "\n",
      "[0]\ttrain-rmse:4.88988\teval-rmse:4.89042\ttrain-mae:3216.45\teval-mae:3222.4\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.548982\teval-rmse:0.556185\ttrain-mae:1312.4\teval-mae:1334.78\n",
      "[100]\ttrain-rmse:0.478417\teval-rmse:0.490461\ttrain-mae:1143.56\teval-mae:1177.87\n",
      "[150]\ttrain-rmse:0.469928\teval-rmse:0.484437\ttrain-mae:1117.8\teval-mae:1158.61\n",
      "[200]\ttrain-rmse:0.466375\teval-rmse:0.482706\ttrain-mae:1106.59\teval-mae:1152.28\n",
      "[250]\ttrain-rmse:0.463961\teval-rmse:0.481708\ttrain-mae:1099\teval-mae:1148.79\n",
      "[300]\ttrain-rmse:0.462221\teval-rmse:0.481067\ttrain-mae:1093.54\teval-mae:1146.4\n",
      "[350]\ttrain-rmse:0.460709\teval-rmse:0.48059\ttrain-mae:1088.87\teval-mae:1144.61\n",
      "[400]\ttrain-rmse:0.459316\teval-rmse:0.480195\ttrain-mae:1084.56\teval-mae:1143.21\n",
      "[450]\ttrain-rmse:0.458156\teval-rmse:0.479945\ttrain-mae:1080.94\teval-mae:1142.4\n",
      "[500]\ttrain-rmse:0.457009\teval-rmse:0.479727\ttrain-mae:1077.3\teval-mae:1141.52\n",
      "[550]\ttrain-rmse:0.455924\teval-rmse:0.479489\ttrain-mae:1073.96\teval-mae:1140.64\n",
      "[600]\ttrain-rmse:0.455088\teval-rmse:0.479326\ttrain-mae:1071.41\teval-mae:1140.08\n",
      "[650]\ttrain-rmse:0.454308\teval-rmse:0.479179\ttrain-mae:1068.99\teval-mae:1139.55\n",
      "[700]\ttrain-rmse:0.453417\teval-rmse:0.479033\ttrain-mae:1066.31\teval-mae:1139.06\n",
      "[750]\ttrain-rmse:0.452636\teval-rmse:0.478874\ttrain-mae:1063.95\teval-mae:1138.54\n",
      "[800]\ttrain-rmse:0.45198\teval-rmse:0.478843\ttrain-mae:1061.98\teval-mae:1138.47\n",
      "[850]\ttrain-rmse:0.451372\teval-rmse:0.478729\ttrain-mae:1060.11\teval-mae:1137.97\n",
      "[900]\ttrain-rmse:0.450716\teval-rmse:0.478671\ttrain-mae:1058.11\teval-mae:1137.77\n",
      "[950]\ttrain-rmse:0.450075\teval-rmse:0.478611\ttrain-mae:1056.24\teval-mae:1137.56\n",
      "[1000]\ttrain-rmse:0.44955\teval-rmse:0.478549\ttrain-mae:1054.54\teval-mae:1137.27\n",
      "[1050]\ttrain-rmse:0.448878\teval-rmse:0.478511\ttrain-mae:1052.51\teval-mae:1137.29\n",
      "[1100]\ttrain-rmse:0.448366\teval-rmse:0.478459\ttrain-mae:1050.91\teval-mae:1137.03\n",
      "[1150]\ttrain-rmse:0.447828\teval-rmse:0.478397\ttrain-mae:1049.31\teval-mae:1136.82\n",
      "[1200]\ttrain-rmse:0.44741\teval-rmse:0.478384\ttrain-mae:1048.09\teval-mae:1136.84\n",
      "[1250]\ttrain-rmse:0.446952\teval-rmse:0.478318\ttrain-mae:1046.65\teval-mae:1136.68\n",
      "[1300]\ttrain-rmse:0.446489\teval-rmse:0.478289\ttrain-mae:1045.34\teval-mae:1136.55\n",
      "[1350]\ttrain-rmse:0.446002\teval-rmse:0.478303\ttrain-mae:1043.78\teval-mae:1136.44\n",
      "[1400]\ttrain-rmse:0.445604\teval-rmse:0.478269\ttrain-mae:1042.42\teval-mae:1136.2\n",
      "[1450]\ttrain-rmse:0.445194\teval-rmse:0.478261\ttrain-mae:1041.18\teval-mae:1136.19\n",
      "[1500]\ttrain-rmse:0.444683\teval-rmse:0.478255\ttrain-mae:1039.7\teval-mae:1136.12\n",
      "[1550]\ttrain-rmse:0.444322\teval-rmse:0.478199\ttrain-mae:1038.6\teval-mae:1136.14\n",
      "Stopping. Best iteration:\n",
      "[1499]\ttrain-rmse:0.444692\teval-rmse:0.478256\ttrain-mae:1039.73\teval-mae:1136.11\n",
      "\n",
      "eval-MAE: 1136.113249\n",
      "\n",
      " Fold 5\n",
      "\n",
      "[0]\ttrain-rmse:4.8895\teval-rmse:4.8875\ttrain-mae:3220.37\teval-mae:3206.64\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.550376\teval-rmse:0.554164\ttrain-mae:1316.94\teval-mae:1317.77\n",
      "[100]\ttrain-rmse:0.479682\teval-rmse:0.487259\ttrain-mae:1147.87\teval-mae:1163.35\n",
      "[150]\ttrain-rmse:0.470855\teval-rmse:0.480688\ttrain-mae:1120.95\teval-mae:1144.61\n",
      "[200]\ttrain-rmse:0.467457\teval-rmse:0.478829\ttrain-mae:1109.92\teval-mae:1138.79\n",
      "[250]\ttrain-rmse:0.464909\teval-rmse:0.477708\ttrain-mae:1101.81\teval-mae:1135.3\n",
      "[300]\ttrain-rmse:0.463037\teval-rmse:0.477054\ttrain-mae:1095.84\teval-mae:1133.29\n",
      "[350]\ttrain-rmse:0.461349\teval-rmse:0.476512\ttrain-mae:1090.77\teval-mae:1131.55\n",
      "[400]\ttrain-rmse:0.460017\teval-rmse:0.476107\ttrain-mae:1086.6\teval-mae:1130.24\n",
      "[450]\ttrain-rmse:0.458677\teval-rmse:0.4758\ttrain-mae:1082.35\teval-mae:1129.25\n",
      "[500]\ttrain-rmse:0.457677\teval-rmse:0.475618\ttrain-mae:1079.21\teval-mae:1128.84\n",
      "[550]\ttrain-rmse:0.456707\teval-rmse:0.475434\ttrain-mae:1076.19\teval-mae:1128.21\n",
      "[600]\ttrain-rmse:0.4559\teval-rmse:0.475314\ttrain-mae:1073.65\teval-mae:1127.83\n",
      "[650]\ttrain-rmse:0.455084\teval-rmse:0.475124\ttrain-mae:1071.17\teval-mae:1127.26\n",
      "[700]\ttrain-rmse:0.454322\teval-rmse:0.475008\ttrain-mae:1068.77\teval-mae:1126.79\n",
      "[750]\ttrain-rmse:0.453634\teval-rmse:0.474927\ttrain-mae:1066.65\teval-mae:1126.42\n",
      "[800]\ttrain-rmse:0.452855\teval-rmse:0.474828\ttrain-mae:1064.35\teval-mae:1126.21\n",
      "[850]\ttrain-rmse:0.452141\teval-rmse:0.474746\ttrain-mae:1062.21\teval-mae:1125.87\n",
      "[900]\ttrain-rmse:0.451536\teval-rmse:0.474688\ttrain-mae:1060.44\teval-mae:1125.66\n",
      "[950]\ttrain-rmse:0.450933\teval-rmse:0.474611\ttrain-mae:1058.39\teval-mae:1125.46\n",
      "[1000]\ttrain-rmse:0.450339\teval-rmse:0.474538\ttrain-mae:1056.61\teval-mae:1125.21\n",
      "[1050]\ttrain-rmse:0.4498\teval-rmse:0.474481\ttrain-mae:1054.97\teval-mae:1125.1\n",
      "[1100]\ttrain-rmse:0.449296\teval-rmse:0.474481\ttrain-mae:1053.46\teval-mae:1125.01\n",
      "[1150]\ttrain-rmse:0.448809\teval-rmse:0.47446\ttrain-mae:1051.95\teval-mae:1124.84\n",
      "[1200]\ttrain-rmse:0.448292\teval-rmse:0.474452\ttrain-mae:1050.33\teval-mae:1124.74\n",
      "[1250]\ttrain-rmse:0.447782\teval-rmse:0.474435\ttrain-mae:1048.77\teval-mae:1124.66\n",
      "[1300]\ttrain-rmse:0.447294\teval-rmse:0.474399\ttrain-mae:1047.37\teval-mae:1124.6\n",
      "[1350]\ttrain-rmse:0.446828\teval-rmse:0.474406\ttrain-mae:1045.99\teval-mae:1124.65\n",
      "[1400]\ttrain-rmse:0.446434\teval-rmse:0.474395\ttrain-mae:1044.74\teval-mae:1124.6\n",
      "Stopping. Best iteration:\n",
      "[1315]\ttrain-rmse:0.44713\teval-rmse:0.474379\ttrain-mae:1046.89\teval-mae:1124.52\n",
      "\n",
      "eval-MAE: 1124.518390\n",
      "\n",
      " Average eval-MAE: 1133.827593\n",
      " Time taken: 1314 minutes and 14.84 seconds.\n",
      "#\n",
      " Writing results\n",
      "\n",
      " 5-fold average prediction:\n",
      "\n",
      "            loss\n",
      "id              \n",
      "4    1509.004028\n",
      "6    1891.303955\n",
      "9   10465.453125\n",
      "12   6656.203125\n",
      "15    811.918274\n",
      "\n",
      " Writing submission: submission_5fold-average-xgb_1133.827593_2016-12-12-01-30.csv\n"
     ]
    }
   ],
   "source": [
    "xgb_model(X_train)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
